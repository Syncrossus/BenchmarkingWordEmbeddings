{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating text8 model\n",
    "#sentences = word2vec.Text8Corpus(\"~/Documents/ScienceDesDonnees/word2vec/text8\")\n",
    "model = word2vec.Word2Vec(sentences, size=300, window=12, min_count=25, workers=3, sg=1, negative=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read function\n",
    "import pickle\n",
    "def read(file):\n",
    "    with open(file, 'rb') as fichier:\n",
    "        mon_depickler = pickle.Unpickler(fichier)\n",
    "        d_recupere = mon_depickler.load()\n",
    "        return d_recupere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# loading text8 model\n",
    "model = read('modeltext8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62197202"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cosine similarity\n",
    "w1 = 'car'\n",
    "w2 = 'automobile'\n",
    "(model.wv[w1].dot(model.wv[w2]))/(np.linalg.norm(model.wv[w1])*np.linalg.norm(model.wv[w2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38240113095977313"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine similarity?\n",
    "model.wv.similarity(w1, w2)\n",
    "# Yes, = relatedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3201885"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euclidean distance\n",
    "np.linalg.norm(model.wv[w1]-model.wv[w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming text8\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "stemmer = LancasterStemmer()\n",
    "sentences = word2vec.Text8Corpus(\"text8\")\n",
    "sentences = [[stemmer.stem(word) for word in sentence] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# viewing text8\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# levenshtein distance\n",
    "def levenshtein(l1, l2):\n",
    "    m = np.zeros(shape=(len(l1),len(l2)))\n",
    "    m[0,:] = range(0,len(l2))\n",
    "    m[:,0] = range(0,len(l1))\n",
    "    for i in range(1,len(l1)):\n",
    "        for j in range(1,len(l2)):\n",
    "            cost = 0\n",
    "            if l1[i]!=l2[j]:\n",
    "                cost = 1\n",
    "            m[i, j] = min(m[i-1, j] + cost, m[i, j-1] + cost, m[i-1, j-1] + cost)\n",
    "    return m[len(l1)-1, len(l2)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some imports\n",
    "import csv\n",
    "import numpy as np\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading human judgement datasets\n",
    "with open(WORDSIM[0], 'rt')as fichier:\n",
    "    reader = csv.reader(fichier, delimiter=WORDSIM[1])\n",
    "    # Concaténation de trois colonnes en un tuple de mots et un score\n",
    "    np.matrix([[(i[0], i[1]), float(i[2])] for i in reader])\n",
    "\n",
    "with open(MC_CSV[0], 'rt')as fichier:\n",
    "    reader = csv.reader(fichier, delimiter=MC_CSV[1])\n",
    "    # Concaténation de trois colonnes en un tuple de mots et un score\n",
    "    np.matrix([[(i[0], i[1]), float(i[2])] for i in reader if any(i)])\n",
    "\n",
    "with open(MTURK[0], 'rt')as fichier:\n",
    "    reader = csv.reader(fichier, delimiter=MTURK[1])\n",
    "    # Concaténation de trois colonnes en un tuple de mots et un score\n",
    "    np.matrix([[(i[0], i[1]), float(i[2])] for i in reader])\n",
    "\n",
    "with open(REL122[0], 'rt')as fichier:\n",
    "    reader = csv.reader(fichier, delimiter=REL122[1])\n",
    "    # Concaténation de trois colonnes en un tuple de mots et un score\n",
    "    np.matrix([[(i[1], i[2]), float(i[0])] for i in reader])\n",
    "\n",
    "with open(RG[0], 'rt')as fichier:\n",
    "    reader = csv.reader(fichier, delimiter=RG[1])\n",
    "    # Concaténation de trois colonnes en un tuple de mots et un score\n",
    "    np.matrix([[(i[0], i[1]), float(i[2])] for i in reader if any(i)])\n",
    "\n",
    "with open(SIMLEX, 'rt')as fichier:\n",
    "    reader = csv.reader(fichier, delimiter='\\t')\n",
    "    next(reader)\n",
    "    np.matrix([[(i[0], i[1]), float(i[3])] for i in reader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-af3d3b2b16ca>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-af3d3b2b16ca>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print(np.matrix([[(i[2], i[3]), float(i[0])] for i in reader])\u001b[0m\n\u001b[1;37m                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "with open(UMNSRS_REL, 'rt')as fichier:\n",
    "    reader = csv.reader(fichier, delimiter=',')\n",
    "    print(np.matrix([[(i[2], i[3]), float(i[0])] for i in reader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.37667335509407629, 0.0),\n",
       " SpearmanrResult(correlation=0.2607654822179622, pvalue=0.0),\n",
       " 10.571573090541223)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating word pairs\n",
    "model.wv.evaluate_word_pairs(pairs=\"C:/Users/Syncrossus/Documents/GitHub/BenchmarkingWordEmbeddings/datasets_human_judgements_exploitable/cos_matrix_brm_IFR.csv\",delimiter=',',case_insensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-67e3f8046781>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Syncrossus/Documents/GitHub/BenchmarkingWordEmbeddings/datasets_human_judgements/google_questions-words.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(self, questions, restrict_vocab, most_similar, case_insensitive)\u001b[0m\n\u001b[0;32m    731\u001b[0m                 \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m                 \u001b[1;31m# find the most likely prediction, ignoring OOV words and input words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 733\u001b[1;33m                 \u001b[0msims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    734\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal_vocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    316\u001b[0m             \u001b[0mnegative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36minit_sims\u001b[1;34m(self, replace)\u001b[0m\n\u001b[0;32m    855\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# evaluating performance on google questions\n",
    "model.wv.accuracy(\"C:/Users/Syncrossus/Documents/GitHub/BenchmarkingWordEmbeddings/datasets_human_judgements/google_questions-words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix=\"C:/Users/Syncrossus/Documents/GitHub/BenchmarkingWordEmbeddings/datasets_human_judgements/\"\n",
    "prefix2=\"C:/Users/Syncrossus/Documents/GitHub/BenchmarkingWordEmbeddings/datasets_human_judgements/\"\n",
    "liste=[prefix+'UMNSRS_relatedness.csv',prefix+'UMNSRS_similarity.csv']\n",
    "listeCopie = [prefix2+'UMNSRS_relatedness2.csv',prefix2+'UMNSRS_similarity2.csv']\n",
    "for j in range(len(liste)):\n",
    "    with open(liste[j] , 'rt') as fichier:\n",
    "        reader = csv.reader(fichier, delimiter=',')\n",
    "        a =[[i[2] ,i[3],i[0]] for i in reader]\n",
    "    with open(listeCopie[j], 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def sauvegarder(file, model):\n",
    "    with open(file, 'wb') as fichier:\n",
    "        mon_pickler = pickle.Pickler(fichier)\n",
    "        mon_pickler.dump(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing glove model\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('glove.840b.300d.converted.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sauvegarder(\"glove_model.pkl\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# importing google model\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7eeab07afa21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msauvegarder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"google_news_model.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-96555fb0f37a>\u001b[0m in \u001b[0;36msauvegarder\u001b[1;34m(file, model)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfichier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mmon_pickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfichier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mmon_pickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sauvegarder(\"google_news_model.pkl\", model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
